{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'1-28026'\n",
    "filenames = []\n",
    "for i in range(1, 28027):\n",
    "    name = \n",
    "    filenames.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict = {}\n",
    "data_dict = {}\n",
    "for i in range(1, 28027):\n",
    "    res = ''\n",
    "    with open('./content/{}.dat'.format(i)) as f:\n",
    "        url = f.readline().strip()\n",
    "        for line in f:\n",
    "            res = res + line\n",
    "    url_dict[i] = url\n",
    "    data_dict[i] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_dict.p', 'wb') as fp:\n",
    "    pickle.dump(data_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_dict.p', 'rb') as fp:\n",
    "    page_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = SnowballStemmer(\"russian\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    text = \" \".join(tokens)\n",
    "#     text = \" \".join([mystem.stem(token) for token in set(tokens)])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28026/28026 [1:01:13<00:00,  7.63it/s] \n"
     ]
    }
   ],
   "source": [
    "textws_dict = {}\n",
    "for i in tqdm(range(1, 28027)):\n",
    "    test = page_dict.get(i) #9610\n",
    "    soup = BeautifulSoup(test, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    text = re.findall(r'[А-Я][а-я]+|[а-я]+', text)\n",
    "    sp = ' '\n",
    "    text = sp.join(text)\n",
    "    text = text.lower()\n",
    "    text = preprocess_text(text)\n",
    "    text = re.sub(r' \\w | \\w\\w ', '', text)\n",
    "    textws_dict[i] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('textws_dict.p', 'wb') as fp:\n",
    "    pickle.dump(textws_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_stem(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "#     text = \" \".join(tokens)\n",
    "    text = \" \".join([mystem.stem(token) for token in tokens])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28026/28026 [4:36:24<00:00,  1.69it/s]   \n"
     ]
    }
   ],
   "source": [
    "text_dict = {}\n",
    "for i in tqdm(range(1, 28027)):\n",
    "    test = page_dict.get(i) #9610\n",
    "    soup = BeautifulSoup(test, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    text = re.findall(r'[А-Я][а-я]+|[а-я]+', text)\n",
    "    sp = ' '\n",
    "    text = sp.join(text)\n",
    "    text = text.lower()\n",
    "    text = preprocess_text_stem(text)\n",
    "    text = re.sub(r' \\w ', '', text)\n",
    "    text_dict[i] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_dict.p', 'wb') as fp:\n",
    "    pickle.dump(text_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
